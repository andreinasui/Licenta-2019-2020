# -*- coding: utf-8 -*-
"""Licenta_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Oh5MSG-E5ONnL6NmXAdv67uxS1Id7wB9
"""

from google.colab import drive, files

drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
# %tensorflow_version 2.x
import tensorflow as tf
from tensorflow import keras
# %load_ext tensorboard
import tensorboard
from datetime import datetime
from sklearn.feature_selection import *
from sklearn import preprocessing
from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler, LabelEncoder
from keras.preprocessing.image import ImageDataGenerator
import math
import statistics

dataset_url = '/content/gdrive/My Drive/colab/processed-mental-states (1).csv'
data = pd.read_csv(dataset_url)
data

# Split columns as features and class
#X = data.iloc[:,14:-1] # features
X = data.iloc[:,0:-1] # features
Y = data.iloc[:,-1] # target class
img_size = tf.constant([20,20])

def SelectFeaturesChi2(X,Y,img_size):
    # Modify X to be >= 0
    # Add global min for every data entry
    global_min = X.min().min()
    X = pd.DataFrame([X[col] + abs(global_min) for col in X.columns]).T

    # Try with row min instead of global min
    #row_min = X.min(axis=1)
    #X = pd.DataFrame([X.iloc[i] + abs(row_min[i]) for i in X.index])
    #print(X.head())

    # Select best feature using chi2 algorithm
    num_features = img_size.numpy()[0]*img_size.numpy()[1]

    bestFeatures = SelectKBest(score_func=chi2,k=num_features)
    fit = bestFeatures.fit(X,Y)

    dfscores = pd.DataFrame(fit.scores_)
    dfcolumns = pd.DataFrame(X.columns)
    featureScores = pd.concat([dfcolumns, dfscores], axis=1)
    featureScores.columns = ['Features', 'Score']

    # Select the top 400 (20 x 20 image matrix dimensions) features, Score >= 10
    top_features = pd.DataFrame(featureScores.nlargest(num_features,'Score')).sort_index()
    # top_features = pd.DataFrame(featureScores.nlargest(num_features,'Score'))
    X = X.filter(top_features.Features,axis=1)

    # Substract global min again to the data, to restore it after feature selection
    X = pd.DataFrame([X[col] - abs(global_min) for col in X.columns]).T
    
    # Try with row min instead of global min
    #X = pd.DataFrame([X.iloc[i] - abs(row_min[i] for i in X.index])

    return X

def SelectFeaturesMutualInfo(X,Y,img_size):
    # Modify X to be >= 0
    # Add global min for every data entry
    #global_min = X.min().min()
    #X = pd.DataFrame([X[col] + abs(global_min) for col in X.columns]).T

    # Select best feature using mutual_info_classif algorithm
    num_features = img_size.numpy()[0]*img_size.numpy()[1]

    bestFeatures = SelectKBest(score_func=mutual_info_classif,k=num_features)
    fit = bestFeatures.fit(X,Y)

    dfscores = pd.DataFrame(fit.scores_)
    dfcolumns = pd.DataFrame(X.columns)
    featureScores = pd.concat([dfcolumns, dfscores], axis=1)
    featureScores.columns = ['Features', 'Score']

    # Select the top features, Score >= 10
    top_features = pd.DataFrame(featureScores.nlargest(num_features,'Score')).sort_index()
    X = X.filter(top_features.Features,axis=1)

    # Substract global min again to the data, to restore it after feature selection
    #X = pd.DataFrame([X[col] - abs(global_min) for col in X.columns]).T
    return X

X = SelectFeaturesChi2(X,Y,img_size)

# Normalize X so we can use the data as a grayscale image
X_norm = X.values
# I should try multiple methods of normalization to see if they matter on training
# 1. MaxAbsScaler
# 2. MinMaxScaler
# scaler = MaxAbsScaler(copy=False)
# scaler = MinMaxScaler(copy=False)
# X_norm = scaler.fit_transform(X_norm)

# X_norm = preprocessing.scale(X_norm, axis=1)
# X_norm = preprocessing.robust_scale(X_norm, axis=0)
X_norm = preprocessing.minmax_scale(X_norm, axis=1)

X = pd.DataFrame(X_norm,columns=X.columns)
print(X)

plt.imshow(X.iloc[2,:].values.reshape(20,20), cmap='gray')
plt.grid(False)
plt.show()

# One Hot Encod labels for use in CCE
Y = keras.utils.to_categorical(Y.values,num_classes=3)

# Split dataset into train and test (70% train and 30% test)
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,train_size=0.9)
X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,train_size=0.8)

X_train = tf.reshape(X_train.values,[-1,img_size.numpy()[0],img_size.numpy()[1],1])
X_test = tf.reshape(X_test.values,[-1,img_size.numpy()[0],img_size.numpy()[1],1])
X_val = tf.reshape(X_val.values,[-1,img_size.numpy()[0],img_size.numpy()[1],1])

Y_train = Y_train
Y_test = Y_test
Y_val = Y_val
print("Train len: {}\nTest len: {}\nVal len: {}".format(Y_train.shape, Y_test.shape, Y_val.shape))

def MindStateNetGood(img_size):
    # Using this model I got the following results on a 20x20x1 image
    # model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0007,amsgrad=True), loss=keras.losses.CategoricalCrossentropy(),metrics=['accuracy'])
    # patience_epoch = 10
    # early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',patience=patience_epoch,verbose=1,restore_best_weights=True)
    # reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',min_lr=0.00001,patience=5, factor=0.1)
    # history = model.fit(X_train, Y_train, 32, 100, validation_data=(X_val,Y_val),callbacks=[reduce_lr])

    # loss: 0.2249 - accuracy: 0.9593 - val_loss: 0.3767 - val_accuracy: 0.9054 - lr: 1.0000e-05
    # Test loss: 0.32166269421577454
    # Test accuracy: 0.9334889054298401

    inputs = keras.Input(shape=(img_size.numpy()[0],img_size.numpy()[1],1))

    x = keras.layers.Conv2D(32,(2,2),padding='same',strides=1,activation='relu')(inputs)
    x = keras.layers.Conv2D(32,(2,2),padding='valid',strides=1,activation='relu')(x)
    x = keras.layers.BatchNormalization()(x)

    x = keras.layers.Dropout(0.3)(x)
    x = keras.layers.Conv2D(64,(2,2),padding='same',strides=2,activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)
    x = keras.layers.Conv2D(64,(2,2),padding='valid',strides=1,activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.02))(x)
    x = keras.layers.BatchNormalization()(x)

    x = keras.layers.Conv2D(128,(2,2),padding='same',strides=2,activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.05))(x)
    x = keras.layers.Dropout(0.25)(x)
    x = keras.layers.Conv2D(256,(2,2),padding='valid',strides=1,activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.02))(x)
    x = keras.layers.BatchNormalization()(x)

    x = keras.layers.Flatten()(x)
    x = keras.layers.Dense(256,activation='relu')(x)
    x = keras.layers.Dropout(0.5)(x)
    x = keras.layers.Dense(3,activation='softmax')(x)

    return keras.Model(inputs, x)

def compute_model(model, loop_cnt):
    i = 0
    avg_acc = 0
    avg_loss = 0
    history = 0

    acc = []
    loss = []

    fit_verbose = 1 if loop_cnt == 1 else 2

    tf.random.set_seed(1)
    model_ = model
    Wsave = model_.get_weights()

    for i in range(loop_cnt):
        print(f"Starting run number {i+1}\n")
        model_.set_weights(Wsave)
        model_.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0007,amsgrad=True), loss=keras.losses.CategoricalCrossentropy(),metrics=['accuracy'])
        logdir = '/content/gdrive/My Drive/colab/logs/fit/' + datetime.now().strftime("%Y%m%d-%H%M%S")
        tensorboad_cb = keras.callbacks.TensorBoard(log_dir=logdir)
        early_stop = keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=10,verbose=1,restore_best_weights=True)
        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',min_lr=0.000007,patience=5, factor=0.07)
        history = model_.fit(X_train, Y_train, 32, 100, verbose=fit_verbose, validation_data=(X_val,Y_val),callbacks=[reduce_lr])

        # Test model on test dataset
        results = model_.evaluate(X_test, Y_test)
        loss.append(results[0])
        acc.append(results[1])
        avg_loss += results[0] / loop_cnt
        avg_acc += results[1] / loop_cnt
        
    for i in range(loop_cnt):
        print(f"Loss {i+1}: {loss[i]}\nAcc {i+1}: {acc[i]}\n")
    print(f"Avg loss: {avg_loss}\nAvg acc: {avg_acc}\n")
    predictions = model_.predict(X_test)
    return predictions, history, avg_loss, avg_acc

model = MindStateNetGood(img_size)
model.summary()
keras.utils.plot_model(model,show_shapes=True)

predictions, history, avg_loss, avg_acc = compute_model(model, 1)

# Evaluating the model through graphs
plt.title('Accuracy graph')
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'validation_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.35, 1])
plt.xlim(left=-0.3)
plt.legend(loc='lower right')
plt.grid(True)
plt.show()
# plt.savefig("acc_graph.png", dpi=100, bbox_inches='tight')
# files.download("/content/acc_graph.png")

plt.figure()
plt.title('Loss graph')
plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label = 'validation_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim([0.009, 1.2])
plt.xlim(left=-0.3)
plt.legend(loc='lower left')
plt.grid(True)
plt.show()
# plt.savefig("loss_graph.png", dpi=100, bbox_inches='tight')
# files.download("/content/loss_graph.png")

true_class_pred = np.argmax(predictions,axis=1)
true_class = np.argmax(Y_test,axis=1)

# Classification report
from sklearn.metrics import classification_report, matthews_corrcoef
target_names = ['neutru','concentrat','relaxat']
print(classification_report(true_class, true_class_pred, target_names=target_names))

import seaborn as sn
from sklearn import metrics
cm = metrics.confusion_matrix(true_class, true_class_pred)

sn.set(font_scale=1.1) # for label size
sn.heatmap(cm, annot=True, annot_kws={"size": 16}, fmt='g', xticklabels=target_names, yticklabels=target_names,cbar=True) # font size
plt.title("Confusion Matrix")
plt.xlabel("Predicted label")
plt.ylabel("True label")
plt.yticks(rotation=45)
plt.show()
# plt.savefig("conf_matrix.png", dpi=100, bbox_inches='tight')
# files.download("/content/conf_matrix.png")